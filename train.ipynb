{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6c9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, gzip, json, random, math, time\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4ca673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched files: 1024\n",
      "Using files: 32\n",
      "D:/Data/AI/LLM/Text/allenai_c4/multilingual\\c4-zh.tfrecord-00000-of-01024.json.gz\n",
      "D:/Data/AI/LLM/Text/allenai_c4/multilingual\\c4-zh.tfrecord-00001-of-01024.json.gz\n",
      "D:/Data/AI/LLM/Text/allenai_c4/multilingual\\c4-zh.tfrecord-00002-of-01024.json.gz\n",
      "D:/Data/AI/LLM/Text/allenai_c4/multilingual\\c4-zh.tfrecord-00003-of-01024.json.gz\n",
      "D:/Data/AI/LLM/Text/allenai_c4/multilingual\\c4-zh.tfrecord-00004-of-01024.json.gz ...\n",
      " D:/Data/AI/LLM/Text/allenai_c4/multilingual\\c4-zh.tfrecord-00031-of-01024.json.gz\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "file_url = os.getenv(\"DATA_PATH\")\n",
    "assert file_url, \"DATA_PATH 没读到，请检查 .env 或环境变量\"\n",
    "\n",
    "pattern = file_url + \"/multilingual/c4-zh.*.json.gz\"\n",
    "\n",
    "all_files = sorted(glob.glob(pattern))\n",
    "print(\"Matched files:\", len(all_files))\n",
    "assert len(all_files) >= 32, f\"文件不够 32 个，目前只有 {len(all_files)}\"\n",
    "\n",
    "data_files = all_files[:32]\n",
    "print(\"Using files:\", len(data_files))\n",
    "print(\"\\n\".join(data_files[:5]), \"...\\n\", data_files[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb305359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['text', 'timestamp', 'url']\n",
      "Sample snippet: {'text': '锘� 浜���88涓�杞藉�姹�绠＄��灞��伴�诲��瑷�浜恒���荤�娴�甯����ヨ�卞�哄腑�藉�￠�㈡�跨��渚�琛��归�浼�锛�浼���澶�姹�绠＄�����姐��淇�杩�璺ㄥ�璐告����璧�渚垮�╁�����虫�跨�����碉�瀹�褰�锛�_�跨��娉�瑙�瑙ｈ��_浜���88涓�杞藉�姹�绠＄��灞��ㄦ�风�绔�\\n浜���88涓�杞藉�姹�绠＄��灞��伴�诲��瑷�浜恒���荤�娴�甯����ヨ�卞�哄腑�藉�￠�㈡�跨��渚�琛��归�浼�锛�浼���澶�姹�绠＄�����姐��淇�杩�璺ㄥ�璐告����璧�渚垮�╁�����虫�跨�����碉\n"
     ]
    }
   ],
   "source": [
    "def peek_one_record(path):\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            return json.loads(line)\n",
    "    return None\n",
    "\n",
    "sample = peek_one_record(data_files[0])\n",
    "print(\"Keys:\", list(sample.keys())[:30])\n",
    "print(\"Sample snippet:\", str(sample)[:300])\n",
    "\n",
    "# 你可以把 TEXT_FIELD 改成真实字段名，比如 \"content\" / \"raw\" / \"text\"\n",
    "TEXT_FIELD = \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5cd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_texts_jsonl_gz(\n",
    "    files,\n",
    "    text_field=\"text\",\n",
    "    max_docs=None,\n",
    "    keep_prob=1.0,\n",
    "    seed=42,\n",
    "    shuffle_files=True,\n",
    "    min_chars=1,\n",
    "):\n",
    "    rng = random.Random(seed)\n",
    "    files = list(files)\n",
    "    if shuffle_files:\n",
    "        rng.shuffle(files)\n",
    "\n",
    "    seen = 0\n",
    "    for fp in files:\n",
    "        with gzip.open(fp, \"rt\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if keep_prob < 1.0 and rng.random() > keep_prob:\n",
    "                    continue\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                txt = obj.get(text_field, None)\n",
    "                if not isinstance(txt, str):\n",
    "                    continue\n",
    "                if len(txt) < min_chars:\n",
    "                    continue\n",
    "                yield txt\n",
    "                seen += 1\n",
    "                if max_docs is not None and seen >= max_docs:\n",
    "                    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c2e97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer ...\n",
      "Vocab size: 4096\n",
      "Specials: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "TOKENIZER_VOCAB_SIZE = 4096\n",
    "\n",
    "# tokenizer 训练采样量：建议先 50k~300k，太大真的会慢\n",
    "TOKENIZER_MAX_DOCS = 200_000\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "tok = ByteLevelBPETokenizer()\n",
    "\n",
    "train_iter = iter_texts_jsonl_gz(\n",
    "    data_files,\n",
    "    text_field=TEXT_FIELD,\n",
    "    max_docs=TOKENIZER_MAX_DOCS,\n",
    "    keep_prob=1.0,\n",
    "    seed=123,\n",
    "    shuffle_files=True,\n",
    "    min_chars=20,\n",
    ")\n",
    "\n",
    "print(\"Training tokenizer ...\")\n",
    "tok.train_from_iterator(\n",
    "    train_iter,\n",
    "    vocab_size=TOKENIZER_VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tok,\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "print(\"Vocab size:\", hf_tokenizer.vocab_size)\n",
    "print(\"Specials:\", hf_tokenizer.special_tokens_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1473d6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to: D:\\Project\\AI\\NanoZhGPT\\NanoZhGPT\\tokenizer\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(\"./NanoZhGPT\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tok_dir = OUT_DIR / \"tokenizer\"\n",
    "tok_dir.mkdir(exist_ok=True)\n",
    "\n",
    "hf_tokenizer.save_pretrained(tok_dir)\n",
    "print(\"Tokenizer saved to:\", tok_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53343558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class TokenBlockDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        files,\n",
    "        tokenizer,\n",
    "        text_field=\"text\",\n",
    "        block_size=512,\n",
    "        max_steps=None,\n",
    "        batch_size=8,\n",
    "        keep_prob=0.05,\n",
    "        seed=42,\n",
    "    ):\n",
    "        self.files = list(files)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_field = text_field\n",
    "        self.block_size = block_size\n",
    "        self.max_steps = max_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        eos_id = self.tokenizer.eos_token_id\n",
    "        buf = []\n",
    "        produced = 0\n",
    "\n",
    "        text_iter = iter_texts_jsonl_gz(\n",
    "            self.files,\n",
    "            text_field=self.text_field,\n",
    "            max_docs=None,\n",
    "            keep_prob=self.keep_prob,\n",
    "            seed=self.seed,\n",
    "            shuffle_files=True,\n",
    "            min_chars=20,\n",
    "        )\n",
    "\n",
    "        for text in text_iter:\n",
    "            ids = self.tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "            if not ids:\n",
    "                continue\n",
    "            ids.append(eos_id)\n",
    "            buf.extend(ids)\n",
    "\n",
    "            while len(buf) >= self.block_size:\n",
    "                block = buf[: self.block_size]\n",
    "                buf = buf[self.block_size :]\n",
    "\n",
    "                x = torch.tensor(block, dtype=torch.long)\n",
    "                yield {\"input_ids\": x, \"labels\": x.clone()}\n",
    "\n",
    "                produced += 1\n",
    "                if self.max_steps is not None and produced >= (self.max_steps * self.batch_size):\n",
    "                    return\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)\n",
    "    labels = torch.stack([b[\"labels\"] for b in batch], dim=0)\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e018ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: xpu\n",
      "use_bf16: True\n"
     ]
    }
   ],
   "source": [
    "def pick_device():\n",
    "    # Intel XPU (torch.xpu)\n",
    "    if hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
    "        return torch.device(\"xpu\")\n",
    "    # CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = pick_device()\n",
    "print(\"Device:\", device)\n",
    "\n",
    "use_bf16 = (device.type == \"xpu\")  # Intel GPU 通常 bf16 比 fp16 更稳\n",
    "print(\"use_bf16:\", use_bf16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a49b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 5.92M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(4096, 256)\n",
       "    (wpe): Embedding(512, 256)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=768, nx=256)\n",
       "          (c_proj): Conv1D(nf=256, nx=256)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=1024, nx=256)\n",
       "          (c_proj): Conv1D(nf=256, nx=1024)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "\n",
    "# 你想更接近 10M：优先改 n_layer（层数）和 n_embd（隐藏维度）\n",
    "MODEL_CFG = dict(\n",
    "    vocab_size=hf_tokenizer.vocab_size,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=256,\n",
    "    n_layer=6,\n",
    "    n_head=4,\n",
    "    bos_token_id=hf_tokenizer.bos_token_id,\n",
    "    eos_token_id=hf_tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "config = GPT2Config(**MODEL_CFG)\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Params: {n_params/1e6:.2f}M\")\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader ready.\n"
     ]
    }
   ],
   "source": [
    "BLOCK_SIZE = 512\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM = 4\n",
    "\n",
    "# 训练时从巨量文本里抽样：0.03~0.15 一般都挺好用\n",
    "TRAIN_KEEP_PROB = 0.06\n",
    "\n",
    "MAX_STEPS = 3000          # 以 step 为单位，不按 epoch（因为数据无限大）\n",
    "LR = 3e-4\n",
    "WARMUP_STEPS = 200\n",
    "WEIGHT_DECAY = 0.1\n",
    "\n",
    "train_ds = TokenBlockDataset(\n",
    "    data_files,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    text_field=TEXT_FIELD,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    max_steps=MAX_STEPS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    keep_prob=TRAIN_KEEP_PROB,\n",
    "    seed=2025,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,  # 如果你 CPU 很强可以试 2~4；但 json.gz 读盘有时反而不稳\n",
    "    pin_memory=(device.type in (\"cuda\",)),\n",
    ")\n",
    "\n",
    "print(\"Dataloader ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a98add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e9b02f20e049d78af2e712780548f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved checkpoint: D:\\Project\\AI\\NanoZhGPT\\NanoZhGPT\\ckpt_step_500\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=MAX_STEPS,\n",
    ")\n",
    "\n",
    "# xpu 的 autocast（没有就退化为普通训练）\n",
    "xpu_autocast = None\n",
    "if device.type == \"xpu\":\n",
    "    try:\n",
    "        from torch.xpu.amp import autocast as xpu_autocast\n",
    "    except Exception:\n",
    "        xpu_autocast = None\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "pbar = tqdm(total=MAX_STEPS, desc=\"train\")\n",
    "t0 = time.time()\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "    if device.type == \"xpu\" and use_bf16 and xpu_autocast is not None:\n",
    "        with xpu_autocast(dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = out.loss / GRAD_ACCUM\n",
    "    elif device.type == \"cuda\":\n",
    "        from torch.cuda.amp import autocast\n",
    "        with autocast():\n",
    "            out = model(**batch)\n",
    "            loss = out.loss / GRAD_ACCUM\n",
    "    else:\n",
    "        out = model(**batch)\n",
    "        loss = out.loss / GRAD_ACCUM\n",
    "\n",
    "    loss.backward()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if (batch_idx + 1) % GRAD_ACCUM == 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        step += 1\n",
    "        if step % 20 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            pbar.set_postfix(loss=f\"{(running_loss/20):.4f}\", lr=f\"{scheduler.get_last_lr()[0]:.2e}\", sec=f\"{elapsed:.1f}\")\n",
    "            running_loss = 0.0\n",
    "            t0 = time.time()\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            ckpt_dir = OUT_DIR / f\"ckpt_step_{step}\"\n",
    "            ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "            model.save_pretrained(ckpt_dir)\n",
    "            hf_tokenizer.save_pretrained(ckpt_dir)\n",
    "            print(f\"\\nSaved checkpoint: {ckpt_dir.resolve()}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "        if step >= MAX_STEPS:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "print(\"Training done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e119e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final model to: D:\\Project\\AI\\NanoZhGPT\\NanoZhGPT\\final\n",
      "今天我们来聊聊大模型训练的基本思路： 月02-21:21:24:07 0::14 昨天: 0:22 8:59::17 2020：55 前天01:49 5: 昨天05:05:07.: 昨天43:15:04:55 前天06 5 3:小时前 6 昨天49:49:06：:06:39\n",
      " 前天16:08:09 昨天:01:\n"
     ]
    }
   ],
   "source": [
    "final_dir = OUT_DIR / \"final\"\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(final_dir)\n",
    "hf_tokenizer.save_pretrained(final_dir)\n",
    "print(\"Saved final model to:\", final_dir.resolve())\n",
    "\n",
    "# 生成测试\n",
    "model.eval()\n",
    "prompt = \"今天我们来聊聊大模型训练的基本思路：\"\n",
    "inputs = hf_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.9,\n",
    "        eos_token_id=hf_tokenizer.eos_token_id,\n",
    "        pad_token_id=hf_tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "print(hf_tokenizer.decode(gen[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb7990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
